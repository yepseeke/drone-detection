{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "390e0191-92c0-4904-bef2-2b70f1a92086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (1.12.0+cu116)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0cb5fa69-2bda-4e88-803a-61ee77163e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from skimage import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3d37afa8-a5a9-4b1a-8bf3-6fb6212d3903",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DroneDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join(self.root_dir, self.annotations.iloc[index, 0])\n",
    "        image = io.imread(img_path)\n",
    "        y_label = torch.tensor(int(self.annotations.iloc[index, 1]))\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return (image, y_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9e975e5c-9818-4f4d-8bc2-204a239df0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "098ed99f-fe1a-49fa-892b-6a261d4eff85",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channel = 3\n",
    "num_classes = 5\n",
    "learning_rate = 1e-3\n",
    "batch_size = 25\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4a7fbea7-6e58-4898-ba7e-88e4ad92838a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DroneDataset('scaleogram.csv', 'drone_dataset_resize', transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ca953799-e4de-40e7-b505-f30bc782fcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = torch.utils.data.random_split(dataset, [6000, 2179])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e05200be-560e-4f95-bc59-e742985d3de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2ebc1e71-4eab-4f26-8e22-17d50f819dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "resnet18 = models.resnet18(pretrained=True)\n",
    "resnet18.fc = nn.Linear(resnet18.fc.in_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4f083c47-e251-496c-9788-3ef5054f725d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "resnet18.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6cd5885d-6085-4a61-9e26-e10f3282d47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(resnet18.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "197f0dfb-fe65-4bd2-93d5-032195824499",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [01:00<19:02, 60.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at epoch 1 is 0.38207545784922936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [01:57<17:35, 58.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at epoch 2 is 0.2391573897950972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [02:56<16:37, 58.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at epoch 3 is 0.18368383756993958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [03:57<15:56, 59.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at epoch 4 is 0.14971279375022278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [05:01<15:16, 61.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at epoch 5 is 0.13314407752283539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [06:00<14:07, 60.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at epoch 6 is 0.14740029296372087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [07:02<13:11, 60.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at epoch 7 is 0.11947934155274803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [08:02<12:06, 60.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at epoch 8 is 0.10317004184180405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [09:03<11:10, 60.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at epoch 9 is 0.10278553498598436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [10:08<10:19, 61.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at epoch 10 is 0.09419090061467918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [11:07<09:10, 61.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at epoch 11 is 0.074071326701475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [12:05<08:01, 60.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at epoch 12 is 0.08337624045625489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [13:09<07:08, 61.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at epoch 13 is 0.06751442998890221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [14:11<06:09, 61.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at epoch 14 is 0.04960429434170995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [15:12<05:06, 61.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at epoch 15 is 0.06147763631342969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [16:12<04:04, 61.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at epoch 16 is 0.0495751044518632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [17:17<03:06, 62.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at epoch 17 is 0.04439307091840116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [18:17<02:03, 61.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at epoch 18 is 0.039777379801186424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [19:18<01:01, 61.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at epoch 19 is 0.02877781282595606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [20:23<00:00, 61.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at epoch 20 is 0.021796014324309\n",
      "Finished Training in  1223.2814421653748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for epoch in tqdm(range(1, num_epochs+1)):\n",
    "    losses = []\n",
    "    resnet18.train()\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        data, targets = data.to(device=device), targets.to(device=device)\n",
    "        \n",
    "        scores = resnet18(data)\n",
    "        loss = criterion(scores, targets)\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f'Cost at epoch {epoch} is {sum(losses) / len(losses)}')\n",
    "        \n",
    "finish = time.time()\n",
    "print('Finished Training in ', finish-start)\n",
    "\n",
    "torch.save(resnet18.state_dict(), './resnet18_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "84197755-e388-4c03-b072-a4a9e5f21044",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    resnet18.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device=device), y.to(device=device)\n",
    "            \n",
    "            scores = resnet18(x)\n",
    "            _, predictions = scores.max(1)\n",
    "            num_correct += (predictions == y).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "            \n",
    "    accuracy = num_correct / num_samples\n",
    "    print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8dd8a605-a798-468a-8119-63593191faf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.20%\n"
     ]
    }
   ],
   "source": [
    "check_accuracy(train_loader, resnet18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b490769b-fa7d-47c1-b4df-56de6535884f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 93.30%\n"
     ]
    }
   ],
   "source": [
    "check_accuracy(test_loader, resnet18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d1de2cf6-d3e8-4a1d-88ab-a6bf8a304f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_single_image(loader, model, image_index):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in enumerate(test_loader):\n",
    "            if i == image_index:\n",
    "                image = images[0].to(device)\n",
    "                label = labels[0].item()\n",
    "\n",
    "                scores = model(image.unsqueeze(0)) \n",
    "\n",
    "                _, prediction = scores.max(1)\n",
    "                prediction = prediction.item()\n",
    "\n",
    "                print(f'Ground Truth Label: {label}')\n",
    "                print(f'Model Prediction: {prediction}')\n",
    "                print(f'Correct Prediction: {prediction == label}')\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ab4fb1-78cb-4b0c-ab41-2e2b9b7070ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in test_loader:\n",
    "    print(f'Batch Size: {images.size(0)}')\n",
    "    print('Labels:', labels)\n",
    "\n",
    "    for i in range(images.size(0)):\n",
    "        image = images[i].numpy().transpose((1, 2, 0))\n",
    "        label = labels[i].item()\n",
    "\n",
    "        print(f'Image {i+1} - Label: {label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6abe8fc5-8fd4-464c-9f98-5be80fef5a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enum(index):\n",
    "    if index == 0:\n",
    "        return 'big drone'\n",
    "    if index == 1:\n",
    "        return 'bird'\n",
    "    if index == 2:\n",
    "        return 'human'\n",
    "    if index == 3:\n",
    "        return 'free space'\n",
    "    if index == 4:\n",
    "        return 'small copter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c034a37e-770b-42c4-a339-174e0f507638",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def test_single_image(model, image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    resized_img = image.resize((590, 390), Image.LANCZOS)\n",
    "\n",
    "    transform = ToTensor()\n",
    "    image_tensor = transform(resized_img).unsqueeze(0)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image_tensor.to(device))\n",
    "\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    predicted_class = predicted.item()\n",
    "    \n",
    "    probabilities = F.softmax(outputs, dim=1)\n",
    "    \n",
    "    return predicted_class, probabilities.squeeze().cpu().numpy()\n",
    "\n",
    "    # print(f'Predicted Class: {predicted_class}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d90c8bcd-2043-456f-8719-d4d41b50f8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class is small copter, probability = 1.0\n"
     ]
    }
   ],
   "source": [
    "predicted_class_number, probability = test_single_image(resnet18, 'image_test/small_copter388.png')\n",
    "predicted_class_name = enum(predicted_class_number)\n",
    "print(f'Predicted class is {predicted_class_name}, probability = {probability[predicted_class_number]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "26216939-a75a-436f-8c7e-bef3f4af7958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/EricCreusen/scaleogram.git\n",
      "  Cloning https://github.com/EricCreusen/scaleogram.git to /tmp/pip-req-build-sb0f3obq\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/EricCreusen/scaleogram.git /tmp/pip-req-build-sb0f3obq\n",
      "  Resolved https://github.com/EricCreusen/scaleogram.git to commit 5804642af123f2f19c60dade3278f5da5fe414e4\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: PyWavelets>=1.0 in /opt/conda/lib/python3.10/site-packages (from scaleogram==0.9.6) (1.3.0)\n",
      "Requirement already satisfied: matplotlib>=2.0 in /opt/conda/lib/python3.10/site-packages (from scaleogram==0.9.6) (3.8.1)\n",
      "Requirement already satisfied: numpy>=1.0 in /opt/conda/lib/python3.10/site-packages (from scaleogram==0.9.6) (1.26.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.0->scaleogram==0.9.6) (0.10.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.0->scaleogram==0.9.6) (2.4.7)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.0->scaleogram==0.9.6) (4.34.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.0->scaleogram==0.9.6) (1.2.0)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.0->scaleogram==0.9.6) (10.1.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.0->scaleogram==0.9.6) (1.4.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.0->scaleogram==0.9.6) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.0->scaleogram==0.9.6) (21.3)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from cycler>=0.10->matplotlib>=2.0->scaleogram==0.9.6) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://github.com/EricCreusen/scaleogram.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "4034bda8-6c83-40de-8498-a933c9ad772f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scaleogram as scg\n",
    "import time\n",
    "\n",
    "from scipy.io import wavfile\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "wavelet = 'cmor1-1.5'\n",
    "coikw = {'alpha': 0.5, 'hatch': '/'}\n",
    "\n",
    "def get_signal(file_path: str):\n",
    "    samplerate, x = wavfile.read(file_path)\n",
    "    return samplerate, x\n",
    "\n",
    "def save_scaleogram(file_path, signal, time, scales, wavelet):\n",
    "    cwt = scg.CWT(time=time, signal=signal, scales=scales)\n",
    "    scg.cws(cwt, figsize=(6, 4), coikw=coikw, wavelet=wavelet, yaxis='frequency', spectrum='amp', title='')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(file_path, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def classify_sound_split(file_path, model):\n",
    "    counter = np.zeros(5)\n",
    "    image_path = 'temp.png'\n",
    "    scales = scg.periods2scales(np.logspace(np.log10(2), np.log10(1000)), wavelet)\n",
    "\n",
    "    sample_rate, signal = get_signal(file_path)\n",
    "    signal_length = signal.shape[0] / sample_rate\n",
    "    if signal_length >= 0.2:\n",
    "        step_size = 0.2\n",
    "        sample_step = int(sample_rate * step_size)\n",
    "        signal_shape = signal.shape[0]\n",
    "\n",
    "        # time = np.linspace(0, signal_length, signal.shape[0])\n",
    "        for i in range(0, signal_shape, sample_step):\n",
    "            start = time.time()\n",
    "            new_signal = signal[i:i + sample_step]\n",
    "            tm = np.linspace(0, step_size, new_signal.shape[0])\n",
    "            save_scaleogram(image_path, new_signal, tm, scales, wavelet)\n",
    "            \n",
    "            predicted_class_number, probability = test_single_image(model, image_path)\n",
    "            print(predicted_class_number, probability)\n",
    "            counter[predicted_class_number] += 1\n",
    "            end = time.time()\n",
    "            print(end - start)\n",
    "    return counter\n",
    "\n",
    "def classify_sound(file_path, model):\n",
    "    image_path = 'temp.png'\n",
    "    scales = scg.periods2scales(np.logspace(np.log10(2), np.log10(1000)), wavelet)\n",
    "    sample_rate, signal = get_signal(file_path)\n",
    "    signal_length = signal.shape[0] / sample_rate\n",
    "    time = np.linspace(0, signal_length, signal.shape[0])\n",
    "    \n",
    "    save_scaleogram(image_path, signal, time, scales, wavelet)\n",
    "            \n",
    "    predicted_class_number, probability = test_single_image(model, image_path)\n",
    "    \n",
    "    return predicted_class_number, probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a9007924-fc7b-451d-9445-24b4bb7b078f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 [4.8134165e-08 3.7009226e-08 1.0000000e+00 6.3155231e-10 8.7476019e-11]\n",
      "1.4550728797912598\n",
      "2 [9.9203987e-07 1.0397373e-04 9.9989426e-01 3.0686687e-07 4.6476916e-07]\n",
      "1.6795954704284668\n",
      "1 [8.2204677e-04 8.0923313e-01 1.8837063e-01 2.9707656e-04 1.2770958e-03]\n",
      "1.4435739517211914\n",
      "2 [2.4170613e-04 2.8920551e-03 9.9683458e-01 1.5169665e-05 1.6514987e-05]\n",
      "1.4458868503570557\n",
      "2 [6.3490884e-09 3.5551940e-07 9.9999964e-01 3.7645798e-10 2.0293465e-11]\n",
      "1.4966843128204346\n",
      "Predicted class is human\n"
     ]
    }
   ],
   "source": [
    "sound_path = 'test_data/sound_human_test2.wav'\n",
    "\n",
    "print(f'Predicted class is {enum(np.argmax(classify_sound_split(sound_path, resnet18)))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "cf2927df-38e9-4407-88c4-a188233d3103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class is human, probability = 0.9995480179786682\n"
     ]
    }
   ],
   "source": [
    "predicted_class_number, probability = classify_sound('sound_human2.wav', resnet18)\n",
    "predicted_class_name = enum(predicted_class_number)\n",
    "print(f'Predicted class is {predicted_class_name}, probability = {probability[predicted_class_number]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c7a122c0-c03d-4d4d-bc6e-19d0d77988e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (10.1.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1a5214f6-cae6-4697-b01f-c3091123c7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image \n",
    "\n",
    "image_folder_path = 'drone_dataset'\n",
    "output_folder_path = 'drone_dataset_resize'\n",
    "\n",
    "image_files = [f for f in os.listdir(image_folder_path) if os.path.isfile(os.path.join(image_folder_path, f))]\n",
    "\n",
    "for image_file in image_files:\n",
    "    image_path = os.path.join(image_folder_path, image_file)\n",
    "    img = Image.open(image_path)\n",
    "    resized_img = img.resize((590, 390), Image.LANCZOS)\n",
    "    output_image_path = os.path.join(output_folder_path, image_file)\n",
    "    \n",
    "    resized_img.save(output_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ce0183-189a-4163-8f25-3a0c3d228517",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
